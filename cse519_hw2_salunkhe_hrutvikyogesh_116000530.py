# -*- coding: utf-8 -*-
"""Hrutvik DSF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IMdORTGXRBcAkTFQA1SlUpeGglBGAOQC

## Copy this notebook (if using Colab) via `File -> Save a Copy in Drive`.

## You can do this assignment outside of Colab (using your local Python installation) via `File -> Download`.

## <u>**Use the "Text" blocks to provide explanations wherever you find them necessary. Highlight your answers inside these text fields to ensure that we don't miss it while grading your HW.**</u>

## **Setup**

- Code to download the data directly from the colab notebook.
- If you find it easier to download the data from the kaggle website (and uploading it to your drive), you can skip this section.
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# First mount your drive before running these cells.
# Create a folder for the this HW and change to that dir
# %cd drive/MyDrive/CSE519

"""# Download data from Kaggle"""

#!pip install -q kaggle

#from google.colab import files
# Create a new API token under "Account" in the kaggle webpage and download the json file
# Upload the file by clicking on the browse
#files.upload()

#! mkdir ~/.kaggle

#! cp kaggle.json ~/.kaggle/

#!kaggle competitions download -c commonlit-evaluate-student-summaries

"""# Alternate: download data using gdown (if having issues with Kaggle)"""

#!pip install gdown

#import gdown
#url = 'https://drive.google.com/uc?id=164sQHZYvxU2XXPokrjzqv9MCGAMHaCIM'
#gdown.download(url)

"""# Extract data and install packages (regardless of data acquisition method)"""

!unzip commonlit-evaluate-student-summaries.zip

### TODO: Install required packages
### Student's code here
!pip install pandas
!pip install scikit-learn
!pip install numpy
!pip install matplotlib
!pip install seaborn
### END

"""## **Section 1: Library and Data Imports (Q1, 5 points)**

- Import your libraries and join the data from both `summaries_train.csv` and `prompts_train.csv` into a single dataframe with the same structure as `use_cols`. Print the head of the dataframe. **Do not modify `use_cols`.**
"""

### TODO: Load required packages
### Student's code here
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
###

use_cols = ["student_id",
            "prompt_id",
            "text",
            "content",
            "wording",
            "prompt_question",
            "prompt_title",
            "prompt_text"
           ]
dtypes = {
        'student_id':                                    'string',
        'prompt_id':                                     'string',
        'text':                                          'string',
        'content':                                       'Float64',
        'wording':                                       'Float64',
        'prompt_question':                               'string',
        'prompt_title':                                  'string',
        'prompt_text':                                   'string',
        }

#Question 1:Load the summaries_train.csv and prompts_train.csv files, joined to replace the prompt_IDs with the relevant text fields into a dataframe. (5 points)

#reading the prompts and summaries training dataset
dtf1=pd.read_csv("prompts_train.csv")
dtf2=pd.read_csv("summaries_train.csv")
#merging the datasets to replace the prompt_IDs with the relevant text fields into a dataframe - merge_dtf
merge_dtf=pd.merge(dtf2,dtf1,on='prompt_id')
merge_dtf.astype(dtypes)
merge_dtf.head()

"""## **Section 2: Features (Q2 and Q3, 25 points total)**"""

# Question 2: Construct a table of five features (really 7) from the text for each instance: (10 points)
# 1)- Number of words in student response (text) and prompt (prompt_text)

#a.)This is for counting the number of words in student response(text) named as word_count
merge_dtf['word_count'] = merge_dtf['text'].apply(lambda x: len(str(x).split()))
merge_dtf.head()

# 1)- b.) This is for counting the number of words in prompt(prompt_text) named as word_count_prompt
merge_dtf['word_count_prompt'] = merge_dtf['prompt_text'].apply(lambda x: len(str(x).split()))
merge_dtf.head()

# 2) Number of distinct words in student response (text) and prompt (prompt_text)

# a.) This is for counting the number of distinct words in student response(text) named as word_distinct
merge_dtf['word_distinct'] = merge_dtf['text'].apply(lambda x: len(set(str(x).split())))

merge_dtf.head()

# 2) b.) This is for counting the number of distinct words in prompt(prompt_text) named as word_distinct2
merge_dtf['word_distinct2'] = merge_dtf['prompt_text'].apply(lambda x: len(set(str(x).split())))

merge_dtf.head()

# 3) Number of words common to student response (text) and prompt (prompt_text)

# This is for counting the number of common or similar words in student response(text) and prompt(prompt_text) named as word_common
def word_common(text, prompt_text):
    wordset1 = set(text.split())
    wordset2 = set(prompt_text.split())
    word_common=wordset1.intersection(wordset2)
    return len(word_common)
merge_dtf['word_common'] = merge_dtf.apply(lambda row: word_common(row['text'], row['prompt_text']), axis=1)
merge_dtf.head()

# 4) Number of words common to student response (text) and prompt_question

# This is for counting the number of common or similar words in student response(text) and prompt questions(prompt_question) named as word_common2
def word_common2(text, prompt_question):
    wordset1 = set(text.split())
    wordset2 = set(prompt_question.split())
    word_common2=wordset1.intersection(wordset2)
    return len(word_common2)
merge_dtf['word_common2'] = merge_dtf.apply(lambda row: word_common2(row['text'], row['prompt_question']), axis=1)
merge_dtf.head()

# 5) Number of words common to student response (text) and prompt_title

# This is for counting the number of common or similar words in student response(text) and title of the prompt(prompt_title) named as word_common3
def word_common3(text, prompt_title):
    wordset1 = set(text.split())
    wordset2 = set(prompt_title.split())
    word_common3=wordset1.intersection(wordset2)
    return len(word_common3)
merge_dtf['word_common3'] = merge_dtf.apply(lambda row: word_common3(row['text'], row['prompt_title']), axis=1)
merge_dtf.head()

# Question-3: Now fortify this list with at least five other numerical features.  Consider readability indices, counts of words from particular classes (e.g character length, part of speech, popularity).  Use your imagination as to what might be helpful for identifying well written summaries of texts. (15 points)

# Here I have added Six more features of my own.
# 1st feature: average word length i.e. finding the average length of words in student response and naming it as average word length
import nltk
wordlists = merge_dtf['text'].apply(lambda x: x.split())
sumlength = wordlists.apply(lambda wordlists: sum(len(word) for word in wordlists))
wordstotal = wordlists.apply(len)
avgwordlength = sumlength / wordstotal
merge_dtf['average word length'] = avgwordlength
merge_dtf.head()

#2nd feature: stopword count i.e. counting the number of stopwords present in student response and naming it as stopword_count
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
nltk.download('punkt')
from nltk.tokenize import word_tokenize

def count_stopwords(text):
    words = word_tokenize(text)
    stopwords_present = set(stopwords.words('english'))
    stopwordcounttotal = sum(1 for word in words if word.lower() in stopwords_present)
    return stopwordcounttotal
merge_dtf['stopword_count'] = merge_dtf['text'].apply(count_stopwords)
merge_dtf.head()

#3rd Feature: Calculating the number of punctuations used in student response(text) and naming it as punctuation_count
import string
def calpunctuationcount(text):
    string.punctuation
    punctcount = 0
    for char in text:
        if char in string.punctuation:
            punctcount += 1
    return punctcount
merge_dtf['punctuation_count'] = merge_dtf['text'].apply(calpunctuationcount)
merge_dtf.head()

#4th Feature: Finding the count of nouns in student response and naming it as nouns_counted
def nouncount(text):
    totwords = text.split()
    countednouns = sum(1 for word in totwords if word[0].isupper())
    return countednouns


merge_dtf['nouns counted'] = merge_dtf['text'].apply(nouncount)
merge_dtf.head()

#5th Feature: I have counted the occurrences of double quotation mark in the text and named it as quote_count(I used this feature just for my understanding and experiment)
def count_quotes(text):
    quote_count = text.count('"')
    return quote_count

merge_dtf['quote_count'] = merge_dtf['text'].apply(count_quotes)
merge_dtf.head()

#6th Feature: Finding out how many conjunctions have been used i.e. conjunctions count and naming it as conjunction_count
def conjunctioncount(text):
    # Define a list of common conjunctions
    conjunctionsgiven = ["and", "but", "or", "nor", "for", "yet", "so","since","if","although","because"]
    # Tokenize the text into words and convert to lowercase
    wordstotalled = text.lower().split()

    # Count the occurrences of conjunctions
    conjunct_count = sum(1 for word in wordstotalled if word in conjunctionsgiven)

    return conjunct_count
merge_dtf['conjunction_count'] = merge_dtf['text'].apply(conjunctioncount)
merge_dtf.head()

#Here I have generated a heatmap for displaying the correlation strength between the features and for my understanding to select features
selected_columns = ['content', 'wording', 'word_count','word_count_prompt','word_distinct','word_distinct2','word_common','word_common2','word_common3','average word length','stopword_count','punctuation_count','nouns counted','quote_count','conjunction_count']
selected_data = merge_dtf[selected_columns]

correlation_ = selected_data.corr()

plt.figure(figsize=(15, 10))
sns.heatmap(correlation_, annot=True, cmap='rocket')
plt.title('Heatmap of Selected Columns')
plt.show()

"""## **Section 3: Content and Wording (Q4, 10 points)**"""

# Question-4: Look at the distributions of scores for content and wording, as histograms and scatterplots? What is the range of values here? How well correlated are they?   Do the shapes of these distributions differ for the different prompts?  (10 points)

#Plotting Histogram for content:

plt.figure(figsize=(11,4))

plt.subplot(1,2,1)
plt.hist(merge_dtf['content'],histtype='bar',alpha=0.6, bins=17, color='cyan',edgecolor='black')
plt.grid(axis='y', linestyle='dashdot', alpha=0.5)
plt.title('Content_Histogram')


#Plotting Histogram for wording:
plt.subplot(1,2,2)
plt.hist(merge_dtf['wording'],histtype='bar',alpha=0.6, bins=17, color='orange', edgecolor='black')
plt.grid(axis='y', linestyle='dashdot', alpha=0.5)
plt.title('Wording_Histogram')



plt.tight_layout()
plt.show()

#Plotting Scatterplot for content vs wording using the seaborn library
sns.scatterplot(data=merge_dtf,x='content', y='wording', c='lightblue', alpha=0.7,marker='o', s=34, edgecolor='black')
plt.show()


#Finding the value of Range for content:
value_min_content = merge_dtf['content'].min()
value_max_content = merge_dtf['content'].max()
range_content = value_max_content - value_min_content


#Finding the value of range for wording:
value_min_wording = merge_dtf['wording'].min()
value_max_wording = merge_dtf['wording'].max()
range_wording = value_max_wording - value_min_wording

#displaying both the values:
print(f"range for content:{range_content}")
print(f"range for wording:{range_wording}")

#Finding the value of Correlation between content and wording:
correlation = merge_dtf['content'].corr(merge_dtf['wording'])
print(f"Correlation is:{correlation}")

#Plotting histograms and scatterplots for different prompts so as to see the difference in shapes of the plot
#histogram for prompt_id 814d6b
plt.figure(figsize=(11,4))
plt.subplot(1,2,1)
datarequired1 = merge_dtf[merge_dtf['prompt_id'] == '814d6b']['content']
plt.hist(datarequired1, bins= 17,color='gray', edgecolor='black')
plt.title(f'814d6b-content')

plt.subplot(1,2,2)
datarequired2 = merge_dtf[merge_dtf['prompt_id'] == '814d6b']['wording']
plt.hist(datarequired2, bins= 17,color='gray', edgecolor='black')
plt.title(f'814d6b-wording')

plt.tight_layout
plt.show()

#scatterplot for prompt_id 814d6b
datarequired3=merge_dtf[merge_dtf['prompt_id'] == '814d6b']
sns.scatterplot(data=datarequired3, x='content', y='wording',c='pink',alpha=0.7,s=34,edgecolor='black')
plt.title(f'814d6b-content vs wording')
plt.show()

#histogram for prompt_id 39c16e
plt.figure(figsize=(11,4))
plt.subplot(1,2,1)
datarequired4 = merge_dtf[merge_dtf['prompt_id'] == '39c16e']['content']
plt.hist(datarequired4, bins= 17,color='green', edgecolor='black')
plt.title(f'39c16e-content')

plt.subplot(1,2,2)
datarequired5 = merge_dtf[merge_dtf['prompt_id'] == '39c16e']['wording']
plt.hist(datarequired5, bins= 17,color='green', edgecolor='black')
plt.title(f'39c16e-wording')
plt.tight_layout
plt.show()

#scatterplot for prompt_id 39c16e
datarequired6=merge_dtf[merge_dtf['prompt_id'] == '39c16e']
sns.scatterplot(data=datarequired6, x='content', y='wording',c='brown',alpha=0.7,s=34,edgecolor='black')
plt.title(f'39c16e-content vs wording')
plt.show()

#histogram for prompt_id 3b9047
plt.figure(figsize=(11,4))
plt.subplot(1,2,1)
datarequired7 = merge_dtf[merge_dtf['prompt_id'] == '3b9047']['content']
plt.hist(datarequired7, bins= 17,color='beige', edgecolor='black')
plt.title(f'3b9047-content')

plt.subplot(1,2,2)
datarequired8 = merge_dtf[merge_dtf['prompt_id'] == '3b9047']['wording']
plt.hist(datarequired8, bins= 17,color='beige', edgecolor='black')
plt.title(f'3b9047-wording')
plt.tight_layout
plt.show()

#scatterplot for prompt_id 3b9047
datarequired9=merge_dtf[merge_dtf['prompt_id'] == '3b9047']
sns.scatterplot(data=datarequired9, x='content', y='wording',c='plum',alpha=0.7,s=34,edgecolor='black')
plt.title(f'3b9047-content vs wording')
plt.show()

#histogram for prompt_id ebad26
plt.figure(figsize=(11,4))
plt.subplot(1,2,1)
datarequired10 = merge_dtf[merge_dtf['prompt_id'] == 'ebad26']['content']
plt.hist(datarequired10, bins= 17,color='tomato', edgecolor='black')
plt.title(f'ebad26-content')

plt.subplot(1,2,2)
datarequired11 = merge_dtf[merge_dtf['prompt_id'] == 'ebad26']['wording']
plt.hist(datarequired11, bins= 17,color='tomato', edgecolor='black')
plt.title(f'ebad26-wording')
plt.tight_layout
plt.show()

#scatterplot for prompt_id ebad26
datarequired12=merge_dtf[merge_dtf['prompt_id'] == 'ebad26']
sns.scatterplot(data=datarequired12, x='content', y='wording',c='darkcyan',alpha=0.7,s=34,edgecolor='black')
plt.title(f'ebad26-content vs wording')
plt.show()

"""After plotting the graphs for all the prompts, we can see that the shapes of the scatterplots are almost same stating that they all show positive correlation.

## **Section 4: Words in Good and Bad Essays (Q5, 10 points)**

Question 5: Which words are over-represented in good essays (as per content and wording) while being under-represented in bad ones?     
Conversely, which words appear disproportionately in the bad essays?   What is an appropriate statistic to use here? (10 points)

---

First I have formed a new dataframe named new_dtf which consists of the original training data and then I preprocessed the contents of the texts inside it to use it ahead for model 1 and model 2 and to use it for question 5
"""

#merging the original dataframe df1 and df2 to create a new dataframe for preprocessing
new_dtf=pd.merge(dtf2,dtf1,on='prompt_id')
new_dtf.astype(dtypes)
new_dtf.head()

# TEXT PRE-PROCESSING:
#Tokenizing the contents of the text for each relevant columns:

def tokenization(giventext):
    # Tokenize the input text
    tokens = word_tokenize(giventext)
    return tokens

new_dtf['text']=new_dtf['text'].apply(tokenization)
new_dtf['prompt_text']=new_dtf['prompt_text'].apply(tokenization)
new_dtf['prompt_question']=new_dtf['prompt_question'].apply(tokenization)
new_dtf['prompt_title']=new_dtf['prompt_title'].apply(tokenization)
new_dtf.head()

#removing the punctuation marks from the relevant columns:
def removepunctuation(ofgiventext):
    punctuation_chars = set(string.punctuation)
    cleaned_text = ''
    for char in ofgiventext:
        if char in punctuation_chars:
            cleaned_text += char

    return  cleaned_text
    new_dtf['text'] = new_dtf['text'].apply(removepunctuation)
    new_dtf['prompt_text'] = new_dtf['prompt_text'].apply(removepunctuation)
    new_dtf['prompt_question'] = new_dtf['prompt_question'].apply(removepunctuation)

new_dtf.head()

#removal of digits from the relevant columns using regular expression:
columnswithoutdigits = ['text', 'prompt_question', 'prompt_text']
new_dtf[columnswithoutdigits] = new_dtf[columnswithoutdigits].replace(to_replace=r'\d', value='', regex=True)

new_dtf.head()

#selecting the same features as earlier except the 'Number of quotes'  for the new dataframe new_dtf

new_dtf['word_count'] = new_dtf['text'].apply(lambda x: len(x))

new_dtf['word_count_prompt'] = new_dtf['prompt_text'].apply(lambda x: len(x))

new_dtf['word_distinct'] = new_dtf['text'].apply(lambda x: len(set(x)))

new_dtf['word_distinct2'] = new_dtf['prompt_text'].apply(lambda x: len(set(x)))


def commonwords(tokens1, tokens2):
    set1 = set(tokens1)
    set2 = set(tokens2)
    common_words = set1.intersection(set2)
    return len(common_words)
new_dtf['word_common'] = new_dtf.apply(lambda row: commonwords(row['text'], row['prompt_text']), axis=1)

new_dtf['word_common2'] = new_dtf.apply(lambda row: commonwords(row['text'], row['prompt_question']), axis=1)

new_dtf['word_common3'] = new_dtf.apply(lambda row: commonwords(row['text'], row['prompt_title']), axis=1)



wordlists = new_dtf['text']
sumlength = wordlists.apply(lambda wordlists: sum(len(word) for word in wordlists))
wordstotal = wordlists.apply(len)
avgwordlength = sumlength / wordstotal
new_dtf['average word length'] = avgwordlength

new_dtf['punctuation_count'] = new_dtf['text'].apply(calpunctuationcount)


def nouncounts(text):
    totwords = text
    countednouns = sum(1 for word in totwords if word[0].isupper())
    return countednouns
new_dtf['nouns counted'] = new_dtf['text'].apply(nouncounts)


def conjunctioncounts(text):
    conjunctionsgiven = ["and", "but", "or", "nor", "for", "yet", "so","since","if","although","because"]
    wordstotalled = text
    conjunct_count = sum(1 for word in wordstotalled if word in conjunctionsgiven)
    return conjunct_count
new_dtf['conjunction_count'] = new_dtf['text'].apply(conjunctioncounts)


def countallstopwords(tokens):
    stopwordsss = set(stopwords.words('english'))
    stopwordcounted = sum(1 for token in tokens if token in stopwordsss)
    return stopwordcounted
new_dtf['stopword__count'] = new_dtf['text'].apply(countallstopwords)

new_dtf.head()

#This heatmap describes the correlation between the features of the preprocessed datafram- new_dtf:
selected_columns = ['content', 'wording', 'word_count','word_count_prompt','word_distinct','word_distinct2','word_common','word_common2','word_common3','average word length','stopword__count','punctuation_count','nouns counted','conjunction_count']
selected_data = new_dtf[selected_columns]

correlation_ = selected_data.corr()

plt.figure(figsize=(15, 10))
sns.heatmap(correlation_, annot=True, cmap='rocket')
plt.title('Heatmap of Selected Columns')
plt.show()

#Question 5: Which words are over-represented in good essays (as per content and wording) while being under-represented in bad ones?  Conversely, which words appear disproportionately in the bad essays?   What is an appropriate statistic to use here? (10 points)
from sklearn.feature_extraction.text import TfidfVectorizer
#first we join the tokenized text into a single string which is separated by ' '
new_dtf['text'] = new_dtf['text'].apply(lambda tokens: ' '.join(tokens))
tfivector = TfidfVectorizer(stop_words='english')
tfimatrix = tfivector.fit_transform(new_dtf['text'])
tfidf = pd.DataFrame(tfimatrix.toarray(), columns=tfivector.get_feature_names_out())


# Calculating the median scores for 'wording' and 'content'
medianwording = new_dtf['wording'].median()
mediancontent = new_dtf['content'].median()

# Filtering essays into good and bad based on median scores
goodessays = tfidf[(new_dtf['content'] > mediancontent) & (new_dtf['wording'] > medianwording)]
badessays = tfidf[(new_dtf['content'] < mediancontent) & (new_dtf['wording'] < medianwording)]

# Calculating the mean scores for good and bad essays
meangoodessays = goodessays.mean()
meanbadessays = badessays.mean()

scoredifference = meangoodessays - meanbadessays
wordsorting = scoredifference.sort_values(ascending=False)

# Selecting the top 30 over-represented and under-represented words
overrepresentedwords = wordsorting.head(30)
underrepresentedwords = wordsorting.tail(30)

print("\nOver-represented Words in Good Essays which are under-represented Words in Bad Essays:")
print(overrepresentedwords)

print("\nunder-represented Words in Good Essays which are over-represented Words in Bad Essays:")
print(underrepresentedwords)

"""## **Section 5: Three Interesting Plots (Q6, 15 points)**"""

#Question-6:Create three plots of your own using the dataset that you think reveal something very interesting. Explain what it is, and anything else you learned from your exploration. (15 points)

#1st plot: Heatmap of Correlation between all the features and columns of new dataframe - new_dtf

selected_columns = ['content', 'wording', 'word_count','word_count_prompt','word_distinct','word_distinct2','word_common','word_common2','word_common3','average word length','stopword__count','punctuation_count','nouns counted','conjunction_count']
selected_data = new_dtf[selected_columns]

correlation_ = selected_data.corr()

plt.figure(figsize=(15, 10))
sns.heatmap(correlation_, annot=True, cmap='rocket')
plt.title('Heatmap of Selected Columns')
plt.show()

"""This heatmap shows the correlation between all the columns of the dataframe, we can use this for determining which columns are strongly co-related and which are not. It helps us in feature selection."""

#2nd plot: histogram for average word length
plt.figure(figsize=(11,4))
plt.subplot(1,2,1)
datarequiredx = new_dtf['average word length']
plt.hist(datarequiredx, bins= 17,color='lavender', edgecolor='black')
plt.title(f'average word length histogram')

#scatterplot between content score and average word length in text
datarequiredxx=merge_dtf
sns.scatterplot(data=datarequiredxx, y='average word length', x='content',c='darkcyan',alpha=0.7,s=34,edgecolor='black')
plt.title(f'content score vs average word length')
plt.show()

"""## **Section 6: Baseline Model (Q7, 10 points)**

Question-7: Now build a baseline model for this task. We will call this Model 0. You will train linear regression models for both content and wording on 80% of the training data and test it on the remaining 20% chosen at random.  Use only the original five features described above.  Report the mean squared error of each model. What do you make of the error rate? (10 points)
"""

# Question-7: Now build a baseline model for this task. We will call this Model 0. You will train linear regression models for both content and wording on 80% of the training data and test it on the remaining 20% chosen at random.  Use only the original five features described above.  Report the mean squared error of each model. What do you make of the error rate? (10 points)
#execution of baseline model0
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error



#Features(A) and Target Variables(b)
A = merge_dtf[['word_count', 'word_count_prompt', 'word_distinct', 'word_distinct2', 'word_common']]
b_content = merge_dtf['content']
b_wording = merge_dtf['wording']

# training data-80% testing data-20%
A_train, A_test, b_content_train, b_content_test, b_wording_train, b_wording_test = train_test_split(
    A, b_content, b_wording, test_size=0.2, random_state=42)

#linear regression models for content and wording
contentmodel = LinearRegression()
contentmodel.fit(A_train, b_content_train)

wordingmodel = LinearRegression()
wordingmodel.fit(A_train, b_wording_train)

#evaluating the models
contentprediction = contentmodel.predict(A_test)
wordingprediction = wordingmodel.predict(A_test)

#mean squared error
contentmse = mean_squared_error(b_content_test, contentprediction)
wordingmse = mean_squared_error(b_wording_test, wordingprediction)

print(f'Mean Squared Error (Content): {contentmse}')
print(f'Mean Squared Error (Wording): {wordingmse}')

"""interpretation from the mean squared error rates:- \The mean square error should be less in order for the model to perform efficiently

## **Section 7: Feature Cleaning and Additional Models (Q8 & Q9, 20 points total)**

Question-8: The basic features as defined above are not really suited for the task.  Features can be preprocessed (or cleaned) to improve them before feeding into the model (e.g. normalize them, do a special treatment of missing values, etc). This can significantly improve the performance of your model. Do preprocessing for all the features (the original five plus the extra you add). Explain what you did. (10 points)
"""

#1st-I found out if there are any missing values
new_dtf.isna().any()

#2nd- numeric features are scaled using the MinMaxScaler
from sklearn.preprocessing import MinMaxScaler

scalingcolumns = ['word_count', 'word_count_prompt', 'word_distinct', 'word_distinct2', 'word_common', 'word_common2', 'word_common3', 'average word length', 'stopword__count', 'punctuation_count', 'nouns counted', 'conjunction_count']
scaler = MinMaxScaler()
scaler.fit(new_dtf[scalingcolumns])
new_dtf[scalingcolumns] = scaler.transform(new_dtf[scalingcolumns])
new_dtf.head()

"""Question-9: For each of the two tasks (content and wording) create two models:
Model 1 should use the cleaned features and linear regression for training.  You can do some (potentially non-linear) scaling to keep the scores in range.
Model 2 should use the cleaned features and an algorithm other than logistic regression (e.g. Random Forest, Nearest Neighbor, etc) for training.
         Compare their performance and explain your reasoning for the differences in their performances. (10 points)

"""

#Training the model 1 using scaled features by linear regression
#model 1:
B = new_dtf[['word_count','word_count_prompt','word_distinct','word_distinct2','word_common','word_common2','word_common3','average word length','stopword__count','punctuation_count','nouns counted','conjunction_count']]
a_content = new_dtf['content']
a_wording = new_dtf['wording']

# training data-80% testing data-20%
B_train, B_test, a_content_train, a_content_test, a_wording_train, a_wording_test = train_test_split(
    B, a_content, a_wording, test_size=0.2, random_state=23)

#linear regression models for content and wording
contentmodel = LinearRegression()
contentmodel.fit(B_train, a_content_train)

wordingmodel = LinearRegression()
wordingmodel.fit(B_train, a_wording_train)

#evaluating the models
contentprediction = contentmodel.predict(B_test)
wordingprediction = wordingmodel.predict(B_test)

#mean squared error
contentmse1 = mean_squared_error(a_content_test, contentprediction)
wordingmse1 = mean_squared_error(a_wording_test, wordingprediction)

print(f'Mean Squared Error (Content): {contentmse1}')
print(f'Mean Squared Error (Wording): {wordingmse1}')

#Training the model 2 using scaled features by Random Forest method
#model 2:
from sklearn.ensemble import RandomForestRegressor
# creating a random forest regressor for content
content_model = RandomForestRegressor(random_state=23)
content_model.fit(B_train, a_content_train)

# creating a random forest regressor for wording
wording_model = RandomForestRegressor(random_state=42)
wording_model.fit(B_train, a_wording_train)

# Evaluating the models
predictingcontent = content_model.predict(B_test)
predicitingwording = wording_model.predict(B_test)

# mean squared error for content and wording
contentinmse = mean_squared_error(a_content_test, predictingcontent)
wordinginmse = mean_squared_error(a_wording_test, predicitingwording)

print(f'Mean Squared Error (Content): {contentinmse}')
print(f'Mean Squared Error (Wording): {wordinginmse}')

model_comparision = {
    'Model': ['Model 0', 'Model 1', 'Model 2'],
    'Content MSE': [contentmse, contentmse1, contentinmse],
    'Wording MSE': [wordingmse, wordingmse1, wordinginmse],
}
compare_dtf = pd.DataFrame(model_comparision)
compare_dtf.set_index('Model', inplace=True)
print(compare_dtf)
compare_dtf.head()

"""The mean squared rate of content in model 0 and model 1 is almost similar but due to feature scaling and preprocessing, the mean squared rate of wording in them has reduced by some extent. Whereas in Model 2 using random forest due to its non-linearity models the complex relationships in data which reduces the mean squared error rate for both wording and content significantly.

## **Section 8: Kaggle Submission Screenshots (Q10, 5 points)**

Public Score:

Private Score:

Kaggle profile link:

Screenshot(s):
"""